{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c14605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROGRAM PREDIKSI STUNTING PADA BALITA\n",
      "============================================================\n",
      "\n",
      "LANGKAH 1: MEMUAT DATA YANG SUDAH DIPREPROCESSING\n",
      "Data berhasil dimuat dengan 70076 baris dan 7 kolom.\n",
      "\n",
      "LANGKAH 2: EKSPLORASI DATA UNTUK KLASIFIKASI STUNTING\n",
      "Kolom yang tersedia dalam dataset:\n",
      "['Umur (bulan)', 'Tinggi Badan', 'Jenis Kelamin Encoded', 'Status Gizi Encoded', 'Jenis Kelamin', 'Status Gizi', 'Kategori Umur']\n",
      "\n",
      "Info dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70076 entries, 0 to 70075\n",
      "Data columns (total 7 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Umur (bulan)           70076 non-null  int64  \n",
      " 1   Tinggi Badan           70076 non-null  float64\n",
      " 2   Jenis Kelamin Encoded  70076 non-null  int64  \n",
      " 3   Status Gizi Encoded    70076 non-null  int64  \n",
      " 4   Jenis Kelamin          70076 non-null  object \n",
      " 5   Status Gizi            70076 non-null  object \n",
      " 6   Kategori Umur          70076 non-null  object \n",
      "dtypes: float64(1), int64(3), object(3)\n",
      "memory usage: 3.7+ MB\n",
      "None\n",
      "\n",
      "Distribusi Status Gizi:\n",
      "Status Gizi Encoded\n",
      "3    17519\n",
      "0    17519\n",
      "2    17519\n",
      "1    17519\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Mapping Status Gizi:\n",
      "  0: normal\n",
      "  1: severely stunted\n",
      "  2: stunted\n",
      "  3: tinggi\n",
      "\n",
      "Membuat target binary untuk prediksi stunting...\n",
      "Menggunakan kolom 'Status Gizi' untuk membuat target...\n",
      "Kategori status gizi yang ditemukan: ['tinggi' 'normal' 'stunted' 'severely stunted']\n",
      "\n",
      "Distribusi target stunting:\n",
      "Normal (0): 35038 (50.0%)\n",
      "Stunting (1): 35038 (50.0%)\n",
      "\n",
      "LANGKAH 3: PERSIAPAN FITUR\n",
      "Fitur yang digunakan (5):\n",
      "  1. Umur (bulan)\n",
      "  2. Tinggi Badan\n",
      "  3. Jenis Kelamin Encoded\n",
      "  4. Jenis Kelamin\n",
      "  5. Kategori Umur\n",
      "\n",
      "Shape data final: X=(70076, 5), y=(70076,)\n",
      "Distribusi target: [35038 35038]\n",
      "\n",
      "LANGKAH 4: MEMBAGI DATA\n",
      "Data latih: 56060 sampel\n",
      "Data uji: 14016 sampel\n",
      "Distribusi kelas pada data latih: [28030 28030]\n",
      "Distribusi kelas pada data uji: [7008 7008]\n",
      "\n",
      "LANGKAH 5: SCALING FITUR\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'perempuan'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17032\\3824108827.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;31m# 5. SCALING FITUR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nLANGKAH 5: SCALING FITUR\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m \u001b[0mX_train_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[0mX_test_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;31m# 6. DEFINISI MODEL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             return_tuple = (\n",
      "\u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1094\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    874\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m         \"\"\"\n\u001b[0;32m    876\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 878\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1469\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1470\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1472\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1473\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m         \"\"\"\n\u001b[0;32m    913\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"n_samples_seen_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         X = self._validate_data(\n\u001b[0m\u001b[0;32m    915\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    629\u001b[0m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1014\u001b[0m                 raise ValueError(\n\u001b[0;32m   1015\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m                 \u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    741\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2149\u001b[0m     def __array__(\n\u001b[0;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2151\u001b[0m     \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2153\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2154\u001b[0m         if (\n\u001b[0;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2156\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'perempuan'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, \n",
    "                           precision_score, recall_score, f1_score, roc_auc_score, \n",
    "                           roc_curve, precision_recall_curve, average_precision_score)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PROGRAM PREDIKSI STUNTING PADA BALITA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. LOAD DATA YANG SUDAH DIPREPROCESSING\n",
    "print(\"\\nLANGKAH 1: MEMUAT DATA YANG SUDAH DIPREPROCESSING\")\n",
    "try:\n",
    "    data = pd.read_csv('data_balita_preprocessed.csv')\n",
    "    print(f\"Data berhasil dimuat dengan {data.shape[0]} baris dan {data.shape[1]} kolom.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File 'data_balita_preprocessed.csv' tidak ditemukan.\")\n",
    "    print(\"Pastikan Anda sudah menjalankan script preprocessing terlebih dahulu.\")\n",
    "    exit()\n",
    "\n",
    "# 2. EKSPLORASI DATA UNTUK KLASIFIKASI STUNTING\n",
    "print(\"\\nLANGKAH 2: EKSPLORASI DATA UNTUK KLASIFIKASI STUNTING\")\n",
    "\n",
    "# Periksa kolom yang tersedia\n",
    "print(\"Kolom yang tersedia dalam dataset:\")\n",
    "print(data.columns.tolist())\n",
    "print(f\"\\nInfo dataset:\")\n",
    "print(data.info())\n",
    "\n",
    "# Periksa distribusi status gizi\n",
    "print(\"\\nDistribusi Status Gizi:\")\n",
    "if 'Status Gizi Encoded' in data.columns:\n",
    "    status_counts = data['Status Gizi Encoded'].value_counts()\n",
    "    print(status_counts)\n",
    "    \n",
    "    # Mapping untuk interpretasi\n",
    "    if 'Status Gizi' in data.columns:\n",
    "        unique_status = data['Status Gizi'].unique()\n",
    "        print(\"\\nMapping Status Gizi:\")\n",
    "        for i, status in enumerate(sorted(unique_status)):\n",
    "            print(f\"  {i}: {status}\")\n",
    "\n",
    "# Buat target binary untuk stunting (0: Normal, 1: Stunting)\n",
    "print(\"\\nMembuat target binary untuk prediksi stunting...\")\n",
    "\n",
    "# Metode 1: Berdasarkan kategori status gizi\n",
    "if 'Status Gizi' in data.columns:\n",
    "    print(\"Menggunakan kolom 'Status Gizi' untuk membuat target...\")\n",
    "    unique_categories = data['Status Gizi'].unique()\n",
    "    print(f\"Kategori status gizi yang ditemukan: {unique_categories}\")\n",
    "    \n",
    "    # Identifikasi kategori yang menunjukkan stunting\n",
    "    stunting_keywords = ['stunting', 'stunted', 'pendek', 'sangat pendek']\n",
    "    \n",
    "    # Buat fungsi untuk mengidentifikasi stunting\n",
    "    def is_stunting(status):\n",
    "        status_lower = str(status).lower()\n",
    "        return any(keyword in status_lower for keyword in stunting_keywords)\n",
    "    \n",
    "    data['Is_Stunting'] = data['Status Gizi'].apply(is_stunting).astype(int)\n",
    "    \n",
    "elif 'Status Gizi Encoded' in data.columns:\n",
    "    print(\"Menggunakan kolom 'Status Gizi Encoded' untuk membuat target...\")\n",
    "    # Jika ada encoding, coba identifikasi pola\n",
    "    status_encoded_counts = data['Status Gizi Encoded'].value_counts()\n",
    "    print(\"Distribusi Status Gizi Encoded:\")\n",
    "    print(status_encoded_counts)\n",
    "    \n",
    "    # Asumsi: nilai yang lebih tinggi menunjukkan kondisi yang lebih buruk\n",
    "    # Atau bisa disesuaikan dengan konteks data\n",
    "    threshold = data['Status Gizi Encoded'].median()\n",
    "    data['Is_Stunting'] = (data['Status Gizi Encoded'] > threshold).astype(int)\n",
    "    \n",
    "else:\n",
    "    print(\"Kolom status gizi tidak ditemukan. Menggunakan Z-score tinggi badan...\")\n",
    "    # Metode 2: Berdasarkan Z-score tinggi badan terhadap umur\n",
    "    if 'Tinggi Badan' in data.columns and 'Umur (bulan)' in data.columns:\n",
    "        # Hitung Z-score sederhana berdasarkan rata-rata tinggi per kelompok umur\n",
    "        data['Height_Z_Score'] = data.groupby('Umur (bulan)')['Tinggi Badan'].transform(\n",
    "            lambda x: (x - x.mean()) / x.std() if x.std() > 0 else 0\n",
    "        )\n",
    "        # Stunting jika Z-score < -2 (standar WHO)\n",
    "        data['Is_Stunting'] = (data['Height_Z_Score'] < -2).astype(int)\n",
    "    else:\n",
    "        print(\"PERINGATAN: Tidak dapat membuat target stunting. Menggunakan distribusi acak untuk demo.\")\n",
    "        # Buat target acak untuk demo (jangan digunakan untuk analisis nyata)\n",
    "        np.random.seed(42)\n",
    "        data['Is_Stunting'] = np.random.choice([0, 1], size=len(data), p=[0.7, 0.3])\n",
    "\n",
    "# Periksa distribusi target yang dihasilkan\n",
    "print(\"\\nDistribusi target stunting:\")\n",
    "stunting_counts = data['Is_Stunting'].value_counts().sort_index()\n",
    "\n",
    "# FIX: Periksa apakah kedua kelas ada sebelum mengakses\n",
    "if 0 in stunting_counts.index:\n",
    "    normal_count = stunting_counts[0]\n",
    "    normal_pct = normal_count/len(data)*100\n",
    "    print(f\"Normal (0): {normal_count} ({normal_pct:.1f}%)\")\n",
    "else:\n",
    "    print(\"Normal (0): 0 (0.0%)\")\n",
    "    normal_count = 0\n",
    "\n",
    "if 1 in stunting_counts.index:\n",
    "    stunting_count = stunting_counts[1]\n",
    "    stunting_pct = stunting_count/len(data)*100\n",
    "    print(f\"Stunting (1): {stunting_count} ({stunting_pct:.1f}%)\")\n",
    "else:\n",
    "    print(\"Stunting (1): 0 (0.0%)\")\n",
    "    stunting_count = 0\n",
    "\n",
    "# Periksa apakah ada kasus stunting\n",
    "if stunting_count == 0:\n",
    "    print(\"\\nPERINGATAN: Tidak ada kasus stunting dalam dataset!\")\n",
    "    print(\"Ini bisa terjadi karena:\")\n",
    "    print(\"1. Dataset hanya berisi data balita dengan status gizi normal\")\n",
    "    print(\"2. Kriteria untuk menentukan stunting perlu disesuaikan\")\n",
    "    print(\"3. Data preprocessing belum menangani kategori stunting dengan benar\")\n",
    "    \n",
    "    # Opsi: Buat beberapa kasus stunting untuk demo\n",
    "    print(\"\\nUntuk keperluan demo, akan dibuat beberapa kasus stunting secara artificial...\")\n",
    "    \n",
    "    # Ambil 20% data dan jadikan stunting berdasarkan tinggi badan terendah\n",
    "    if 'Tinggi Badan' in data.columns:\n",
    "        lowest_height_indices = data.nsmallest(int(0.2 * len(data)), 'Tinggi Badan').index\n",
    "        data.loc[lowest_height_indices, 'Is_Stunting'] = 1\n",
    "        \n",
    "        # Update distribusi\n",
    "        stunting_counts = data['Is_Stunting'].value_counts().sort_index()\n",
    "        print(f\"Setelah penyesuaian:\")\n",
    "        if 0 in stunting_counts.index:\n",
    "            print(f\"Normal (0): {stunting_counts[0]} ({stunting_counts[0]/len(data)*100:.1f}%)\")\n",
    "        if 1 in stunting_counts.index:\n",
    "            print(f\"Stunting (1): {stunting_counts[1]} ({stunting_counts[1]/len(data)*100:.1f}%)\")\n",
    "\n",
    "# Periksa lagi apakah masih tidak ada kasus stunting\n",
    "final_stunting_count = (data['Is_Stunting'] == 1).sum()\n",
    "if final_stunting_count == 0:\n",
    "    print(\"KESALAHAN: Masih tidak ada kasus stunting. Program tidak dapat dilanjutkan.\")\n",
    "    print(\"Silakan periksa data dan kriteria penentuan stunting.\")\n",
    "    exit()\n",
    "\n",
    "# 3. PERSIAPAN FITUR\n",
    "print(\"\\nLANGKAH 3: PERSIAPAN FITUR\")\n",
    "\n",
    "# Pilih fitur yang relevan untuk prediksi\n",
    "feature_columns = []\n",
    "exclude_columns = ['Is_Stunting', 'Status Gizi', 'Status Gizi Encoded', 'Height_Z_Score']\n",
    "\n",
    "for col in data.columns:\n",
    "    if col not in exclude_columns:\n",
    "        # Hindari fitur yang bocor informasi target\n",
    "        if not col.startswith('Status Gizi_'):\n",
    "            feature_columns.append(col)\n",
    "\n",
    "print(f\"Fitur yang digunakan ({len(feature_columns)}):\")\n",
    "for i, col in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "# Siapkan X dan y\n",
    "X = data[feature_columns]\n",
    "y = data['Is_Stunting']\n",
    "\n",
    "# Handle missing values jika ada\n",
    "missing_values = X.isnull().sum().sum()\n",
    "if missing_values > 0:\n",
    "    print(f\"\\nMenangani {missing_values} missing values...\")\n",
    "    # Untuk kolom numerik, isi dengan median\n",
    "    numeric_columns = X.select_dtypes(include=[np.number]).columns\n",
    "    X[numeric_columns] = X[numeric_columns].fillna(X[numeric_columns].median())\n",
    "    \n",
    "    # Untuk kolom kategorikal, isi dengan modus\n",
    "    categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        X[col] = X[col].fillna(X[col].mode()[0] if not X[col].mode().empty else 'Unknown')\n",
    "\n",
    "print(f\"\\nShape data final: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Distribusi target: {np.bincount(y)}\")\n",
    "\n",
    "# Periksa apakah ada variasi dalam target\n",
    "if len(np.unique(y)) < 2:\n",
    "    print(\"KESALAHAN: Target hanya memiliki satu kelas. Klasifikasi tidak dapat dilakukan.\")\n",
    "    exit()\n",
    "\n",
    "# 4. SPLIT DATA\n",
    "print(\"\\nLANGKAH 4: MEMBAGI DATA\")\n",
    "\n",
    "# Periksa apakah stratify bisa dilakukan\n",
    "min_class_count = min(np.bincount(y))\n",
    "if min_class_count < 2:\n",
    "    print(\"PERINGATAN: Salah satu kelas memiliki sampel terlalu sedikit untuk stratifikasi.\")\n",
    "    stratify_param = None\n",
    "else:\n",
    "    stratify_param = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=stratify_param\n",
    ")\n",
    "\n",
    "print(f\"Data latih: {X_train.shape[0]} sampel\")\n",
    "print(f\"Data uji: {X_test.shape[0]} sampel\")\n",
    "print(f\"Distribusi kelas pada data latih: {np.bincount(y_train)}\")\n",
    "print(f\"Distribusi kelas pada data uji: {np.bincount(y_test)}\")\n",
    "\n",
    "# Periksa apakah data test memiliki kedua kelas\n",
    "if len(np.unique(y_test)) < 2:\n",
    "    print(\"PERINGATAN: Data test tidak memiliki kedua kelas. Hasil evaluasi mungkin tidak akurat.\")\n",
    "\n",
    "# 5. SCALING FITUR\n",
    "print(\"\\nLANGKAH 5: SCALING FITUR\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 6. DEFINISI MODEL\n",
    "print(\"\\nLANGKAH 6: INISIALISASI MODEL\")\n",
    "\n",
    "# Handle class imbalance\n",
    "try:\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "    print(f\"Class weights: {class_weight_dict}\")\n",
    "except:\n",
    "    print(\"Tidak dapat menghitung class weights. Menggunakan weights default.\")\n",
    "    class_weight_dict = None\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, class_weight='balanced', probability=True),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "print(f\"Model yang akan diuji: {len(models)}\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# 7. TRAINING DAN EVALUASI MODEL\n",
    "print(\"\\nLANGKAH 7: TRAINING DAN EVALUASI MODEL\")\n",
    "\n",
    "results = {}\n",
    "cv_scores = {}\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=min(5, min_class_count), shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Tentukan data yang akan digunakan (scaled atau tidak)\n",
    "        if name in ['SVM', 'Logistic Regression', 'K-Nearest Neighbors']:\n",
    "            X_train_model = X_train_scaled\n",
    "            X_test_model = X_test_scaled\n",
    "        else:\n",
    "            X_train_model = X_train\n",
    "            X_test_model = X_test\n",
    "        \n",
    "        # Training\n",
    "        model.fit(X_train_model, y_train)\n",
    "        \n",
    "        # Prediksi\n",
    "        y_pred = model.predict(X_test_model)\n",
    "        y_pred_proba = model.predict_proba(X_test_model)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Evaluasi dengan zero_division handling\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        # Cross-validation\n",
    "        try:\n",
    "            cv_score = cross_val_score(model, X_train_model, y_train, cv=cv, scoring='f1')\n",
    "            cv_scores[name] = cv_score\n",
    "            cv_mean = cv_score.mean()\n",
    "            cv_std = cv_score.std()\n",
    "        except:\n",
    "            cv_scores[name] = np.array([f1])  # Fallback ke F1 score biasa\n",
    "            cv_mean = f1\n",
    "            cv_std = 0\n",
    "        \n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1-Score: {f1:.4f}\")\n",
    "        print(f\"  CV F1-Score: {cv_mean:.4f} (+/- {cv_std * 2:.4f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error training {name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "if not results:\n",
    "    print(\"KESALAHAN: Tidak ada model yang berhasil dilatih.\")\n",
    "    exit()\n",
    "\n",
    "# 8. PERBANDINGAN MODEL\n",
    "print(\"\\nLANGKAH 8: PERBANDINGAN MODEL\")\n",
    "\n",
    "# Buat DataFrame untuk perbandingan\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[name]['accuracy'] for name in results.keys()],\n",
    "    'Precision': [results[name]['precision'] for name in results.keys()],\n",
    "    'Recall': [results[name]['recall'] for name in results.keys()],\n",
    "    'F1-Score': [results[name]['f1_score'] for name in results.keys()],\n",
    "    'CV F1-Score': [cv_scores[name].mean() for name in results.keys()]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
    "print(\"\\nPerbandingan Performa Model (diurutkan berdasarkan F1-Score):\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Model terbaik\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = results[best_model_name]['model']\n",
    "print(f\"\\nModel terbaik: {best_model_name}\")\n",
    "print(f\"F1-Score: {comparison_df.iloc[0]['F1-Score']:.4f}\")\n",
    "\n",
    "# 9. ANALISIS DETAIL MODEL TERBAIK\n",
    "print(f\"\\nLANGKAH 9: ANALISIS DETAIL MODEL TERBAIK ({best_model_name})\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, results[best_model_name]['y_pred']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, results[best_model_name]['y_pred'])\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# 10. VISUALISASI\n",
    "print(\"\\nLANGKAH 10: VISUALISASI HASIL\")\n",
    "\n",
    "# Plot 1: Perbandingan model\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0, 0].bar(comparison_df['Model'], comparison_df['Accuracy'])\n",
    "axes[0, 0].set_title('Accuracy Comparison')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# F1-Score comparison\n",
    "axes[0, 1].bar(comparison_df['Model'], comparison_df['F1-Score'])\n",
    "axes[0, 1].set_title('F1-Score Comparison')\n",
    "axes[0, 1].set_ylabel('F1-Score')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])\n",
    "axes[1, 0].set_title(f'Confusion Matrix - {best_model_name}')\n",
    "axes[1, 0].set_xlabel('Predicted')\n",
    "axes[1, 0].set_ylabel('Actual')\n",
    "\n",
    "# ROC Curve (jika ada probability)\n",
    "if results[best_model_name]['y_pred_proba'] is not None:\n",
    "    fpr, tpr, _ = roc_curve(y_test, results[best_model_name]['y_pred_proba'])\n",
    "    auc_score = roc_auc_score(y_test, results[best_model_name]['y_pred_proba'])\n",
    "    axes[1, 1].plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
    "    axes[1, 1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    axes[1, 1].set_xlabel('False Positive Rate')\n",
    "    axes[1, 1].set_ylabel('True Positive Rate')\n",
    "    axes[1, 1].set_title('ROC Curve')\n",
    "    axes[1, 1].legend()\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'ROC Curve not available\\n(No probability output)', \n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('ROC Curve')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_evaluation_plots.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Grafik evaluasi model tersimpan sebagai 'model_evaluation_plots.png'\")\n",
    "\n",
    "# 11. FEATURE IMPORTANCE (untuk model yang mendukung)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(f\"\\nLANGKAH 11: FEATURE IMPORTANCE ({best_model_name})\")\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 fitur paling penting:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 15 Feature Importance - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Grafik feature importance tersimpan sebagai 'feature_importance.png'\")\n",
    "\n",
    "# 12. HYPERPARAMETER TUNING UNTUK MODEL TERBAIK\n",
    "print(f\"\\nLANGKAH 12: HYPERPARAMETER TUNING ({best_model_name})\")\n",
    "\n",
    "# Parameter grid berdasarkan model terbaik\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "if best_model_name in param_grids:\n",
    "    print(f\"Melakukan grid search untuk {best_model_name}...\")\n",
    "    \n",
    "    # Tentukan data yang akan digunakan\n",
    "    if best_model_name in ['SVM', 'Logistic Regression', 'K-Nearest Neighbors']:\n",
    "        X_train_tune = X_train_scaled\n",
    "        X_test_tune = X_test_scaled\n",
    "    else:\n",
    "        X_train_tune = X_train\n",
    "        X_test_tune = X_test\n",
    "    \n",
    "    # Grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        models[best_model_name], \n",
    "        param_grids[best_model_name],\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_tune, y_train)\n",
    "    \n",
    "    # Model terbaik setelah tuning\n",
    "    best_tuned_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Parameter terbaik: {grid_search.best_params_}\")\n",
    "    print(f\"CV F1-Score terbaik: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluasi model yang sudah di-tune\n",
    "    y_pred_tuned = best_tuned_model.predict(X_test_tune)\n",
    "    \n",
    "    tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "    tuned_precision = precision_score(y_test, y_pred_tuned)\n",
    "    tuned_recall = recall_score(y_test, y_pred_tuned)\n",
    "    tuned_f1 = f1_score(y_test, y_pred_tuned)\n",
    "    \n",
    "    print(f\"\\nPerforma setelah tuning:\")\n",
    "    print(f\"  Accuracy: {tuned_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {tuned_precision:.4f}\")\n",
    "    print(f\"  Recall: {tuned_recall:.4f}\")\n",
    "    print(f\"  F1-Score: {tuned_f1:.4f}\")\n",
    "    \n",
    "    # Bandingkan dengan model sebelum tuning\n",
    "    print(f\"\\nPerbandingan:\")\n",
    "    print(f\"  F1-Score sebelum tuning: {results[best_model_name]['f1_score']:.4f}\")\n",
    "    print(f\"  F1-Score setelah tuning: {tuned_f1:.4f}\")\n",
    "    print(f\"  Peningkatan: {tuned_f1 - results[best_model_name]['f1_score']:.4f}\")\n",
    "    \n",
    "    # Simpan model terbaik\n",
    "    final_model = best_tuned_model\n",
    "else:\n",
    "    final_model = best_model\n",
    "    print(f\"Grid search tidak tersedia untuk {best_model_name}\")\n",
    "\n",
    "# 13. SIMPAN MODEL DAN HASIL\n",
    "print(\"\\nLANGKAH 13: MENYIMPAN MODEL DAN HASIL\")\n",
    "\n",
    "# Simpan hasil evaluasi\n",
    "comparison_df.to_csv('model_comparison_results.csv', index=False)\n",
    "print(\"Hasil perbandingan model tersimpan sebagai 'model_comparison_results.csv'\")\n",
    "\n",
    "# Simpan model terbaik menggunakan joblib\n",
    "import joblib\n",
    "joblib.dump(final_model, 'best_stunting_model.pkl')\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "print(\"Model terbaik tersimpan sebagai 'best_stunting_model.pkl'\")\n",
    "print(\"Scaler tersimpan sebagai 'feature_scaler.pkl'\")\n",
    "\n",
    "# Simpan daftar fitur\n",
    "feature_info = {\n",
    "    'features': feature_columns,\n",
    "    'model_name': best_model_name,\n",
    "    'performance': {\n",
    "        'accuracy': tuned_accuracy if 'tuned_accuracy' in locals() else results[best_model_name]['accuracy'],\n",
    "        'precision': tuned_precision if 'tuned_precision' in locals() else results[best_model_name]['precision'],\n",
    "        'recall': tuned_recall if 'tuned_recall' in locals() else results[best_model_name]['recall'],\n",
    "        'f1_score': tuned_f1 if 'tuned_f1' in locals() else results[best_model_name]['f1_score']\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('model_info.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "print(\"Informasi model tersimpan sebagai 'model_info.json'\")\n",
    "\n",
    "# 14. CONTOH PREDIKSI\n",
    "print(\"\\nLANGKAH 14: CONTOH PREDIKSI\")\n",
    "\n",
    "# Ambil beberapa sampel dari data test\n",
    "sample_indices = np.random.choice(X_test.index, size=5, replace=False)\n",
    "sample_data = X_test.loc[sample_indices]\n",
    "sample_actual = y_test.loc[sample_indices]\n",
    "\n",
    "# Tentukan data yang akan digunakan untuk prediksi\n",
    "if best_model_name in ['SVM', 'Logistic Regression', 'K-Nearest Neighbors']:\n",
    "    sample_scaled = scaler.transform(sample_data)\n",
    "    sample_pred = final_model.predict(sample_scaled)\n",
    "    if hasattr(final_model, 'predict_proba'):\n",
    "        sample_proba = final_model.predict_proba(sample_scaled)[:, 1]\n",
    "    else:\n",
    "        sample_proba = None\n",
    "else:\n",
    "    sample_pred = final_model.predict(sample_data)\n",
    "    if hasattr(final_model, 'predict_proba'):\n",
    "        sample_proba = final_model.predict_proba(sample_data)[:, 1]\n",
    "    else:\n",
    "        sample_proba = None\n",
    "\n",
    "print(\"Contoh prediksi pada 5 sampel data test:\")\n",
    "print(\"-\" * 60)\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    actual_label = \"Stunting\" if sample_actual.iloc[i] == 1 else \"Normal\"\n",
    "    pred_label = \"Stunting\" if sample_pred[i] == 1 else \"Normal\"\n",
    "    \n",
    "    print(f\"Sampel {i+1}:\")\n",
    "    print(f\"  Aktual: {actual_label}\")\n",
    "    print(f\"  Prediksi: {pred_label}\")\n",
    "    if sample_proba is not None:\n",
    "        print(f\"  Probabilitas Stunting: {sample_proba[i]:.3f}\")\n",
    "    print(f\"  Status: {'✓ Benar' if sample_actual.iloc[i] == sample_pred[i] else '✗ Salah'}\")\n",
    "    print()\n",
    "\n",
    "# RINGKASAN AKHIR\n",
    "print(\"=\" * 60)\n",
    "print(\"RINGKASAN HASIL MODELING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset: {len(data)} sampel\")\n",
    "print(f\"Fitur: {len(feature_columns)} fitur\")\n",
    "print(f\"Target: Prediksi Stunting (Binary Classification)\")\n",
    "print(f\"Model terbaik: {best_model_name}\")\n",
    "print(f\"Performa terbaik:\")\n",
    "final_performance = feature_info['performance']\n",
    "print(f\"  - Accuracy: {final_performance['accuracy']:.4f}\")\n",
    "print(f\"  - Precision: {final_performance['precision']:.4f}\")\n",
    "print(f\"  - Recall: {final_performance['recall']:.4f}\")\n",
    "print(f\"  - F1-Score: {final_performance['f1_score']:.4f}\")\n",
    "print(f\"\\nFile yang dihasilkan:\")\n",
    "print(\"  - best_stunting_model.pkl (model terbaik)\")\n",
    "print(\"  - feature_scaler.pkl (scaler fitur)\")\n",
    "print(\"  - model_info.json (informasi model)\")\n",
    "print(\"  - model_comparison_results.csv (perbandingan model)\")\n",
    "print(\"  - model_evaluation_plots.png (visualisasi evaluasi)\")\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(\"  - feature_importance.png (importance fitur)\")\n",
    "print(\"\\nModeling selesai!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
